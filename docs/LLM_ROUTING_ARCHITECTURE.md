# Advanced LLM Provider Integration - Architecture

**Document Version:** 1.0.0  
**Status:** Draft  
**Last Updated:** December 2024

---

## ?? Architecture Overview

This document describes the technical architecture for the Advanced LLM Provider Integration feature, including component design, data flow, integration points, and implementation details.

### High-Level Architecture

```
???????????????????????????????????????????????????????????????????
?                        Application Layer                         ?
?  (Agents, Recording Service, Test Generator)                    ?
???????????????????????????????????????????????????????????????????
                         ? ILLMProvider
???????????????????????????????????????????????????????????????????
?                    LLM Routing Layer                             ?
?  ????????????????????????????????????????????????????????????  ?
?  ?  RoutingLLMProvider                                       ?  ?
?  ?  - Task Detection                                         ?  ?
?  ?  - Strategy Selection                                     ?  ?
?  ?  - Provider Selection                                     ?  ?
?  ????????????????????????????????????????????????????????????  ?
?               ?                                                  ?
?  ????????????????????????????????????????????????????????????? ?
?  ?  CircuitBreakerLLMProvider                                ? ?
?  ?  - Failure Detection                                      ? ?
?  ?  - State Management (Closed/Open/Half-Open)              ? ?
?  ?  - Automatic Failover                                    ? ?
?  ????????????????????????????????????????????????????????????? ?
?????????????????????????????????????????????????????????????????
                ?
?????????????????????????????????????????????????????????????????
?                    Provider Layer                              ?
?  ????????????????  ????????????????  ????????????????        ?
?  ? Azure OpenAI ?  ?    Ollama    ?  ?    Claude    ?        ?
?  ?   Provider   ?  ?   Provider   ?  ?   Provider   ?        ?
?  ????????????????  ????????????????  ????????????????        ?
??????????????????????????????????????????????????????????????????
                ?                ?              ?
?????????????????????????????????????????????????????????????????
?                    External Services                           ?
?  ????????????????  ????????????????  ????????????????        ?
?  ?    GPT-5     ?  ?  Qwen 2.5-7b ?  ? Claude Sonnet?        ?
?  ? Azure OpenAI ?  ? Local Ollama ?  ?  Anthropic   ?        ?
?  ????????????????  ????????????????  ????????????????        ?
??????????????????????????????????????????????????????????????????
```

---

## ??? Component Architecture

### 1. Routing Layer

#### 1.1 RoutingLLMProvider

**Purpose:** Route LLM requests to appropriate models based on task type and strategy.

**Responsibilities:**
- Detect task type from request content
- Select routing strategy (TaskBased, CostOptimized, etc.)
- Choose primary and fallback providers
- Execute request through circuit breaker
- Track routing decisions for analytics

**Class Design:**
```csharp
public sealed class RoutingLLMProvider : ILLMProvider
{
    private readonly IServiceProvider _serviceProvider;
    private readonly ILLMProviderFactory _providerFactory;
    private readonly LLMRoutingOptions _options;
    private readonly ILogger<RoutingLLMProvider> _logger;
    
    // Core methods
    public async Task<LLMResponse> CompleteAsync(LLMRequest request, CancellationToken ct)
    {
        var taskType = DetectTaskType(request);
        var route = SelectRoute(taskType);
        var provider = GetProvider(route.Primary);
        
        return await ExecuteWithCircuitBreaker(provider, request, route.Fallback, ct);
    }
    
    public async IAsyncEnumerable<string> CompleteStreamAsync(
        LLMRequest request, 
        CancellationToken ct)
    {
        var taskType = DetectTaskType(request);
        var route = SelectRoute(taskType);
        var provider = GetProvider(route.Primary);
        
        await foreach (var token in provider.CompleteStreamAsync(request, ct))
        {
            yield return token;
        }
    }
    
    // Helper methods
    private TaskType DetectTaskType(LLMRequest request);
    private RouteInfo SelectRoute(TaskType taskType);
    private ILLMProvider GetProvider(string providerName);
    private async Task<LLMResponse> ExecuteWithCircuitBreaker(...);
}
```

**Task Detection Algorithm:**
```csharp
private TaskType DetectTaskType(LLMRequest request)
{
    // 1. Check explicit task type in request metadata
    if (request.Metadata.ContainsKey("TaskType"))
        return ParseTaskType(request.Metadata["TaskType"]);
    
    // 2. Analyze system prompt for keywords
    var systemPrompt = request.Messages
        .FirstOrDefault(m => m.Role == MessageRole.System)?.Content ?? "";
    
    if (systemPrompt.Contains("plan", StringComparison.OrdinalIgnoreCase))
        return TaskType.Planning;
    
    if (systemPrompt.Contains("code", StringComparison.OrdinalIgnoreCase) ||
        systemPrompt.Contains("generate test", StringComparison.OrdinalIgnoreCase))
        return TaskType.CodeGeneration;
    
    if (systemPrompt.Contains("analyze", StringComparison.OrdinalIgnoreCase))
        return TaskType.Analysis;
    
    // 3. Analyze user prompt length and complexity
    var userPrompt = request.Messages
        .FirstOrDefault(m => m.Role == MessageRole.User)?.Content ?? "";
    
    if (userPrompt.Length > 1000)
        return TaskType.LongFormGeneration;
    
    // 4. Default based on strategy
    return TaskType.General;
}
```

---

#### 1.2 CircuitBreakerLLMProvider

**Purpose:** Prevent cascading failures by detecting unhealthy providers and routing to fallbacks.

**Responsibilities:**
- Track failure rates per provider
- Manage circuit breaker states
- Automatic failover to fallback provider
- Periodic health checks for recovery
- Emit telemetry events

**State Machine:**
```
         ???????????????
    ??????   CLOSED    ??????
    ?    ?  (Normal)   ?    ? 5 failures
    ?    ???????????????    ? within window
    ?           ?            ?
    ?           ?            ?
    ?    Success?     ???????????????
    ?           ?     ?    OPEN     ?
    ?    ??????????????  (Failing)  ?
    ?    ?            ???????????????
    ?    ?                   ?
    ?    ?                   ? After timeout (30s)
    ?    ?                   ?
    ?    ?            ???????????????
    ?    ?            ? HALF-OPEN   ?
    ?    ?            ?  (Testing)  ?
    ?    ?            ???????????????
    ?    ?                   ?
    ?    ?        Success    ? Failure
    ?    ??????????????????????????????
    ???????????????????????????????????
```

**Class Design:**
```csharp
public sealed class CircuitBreakerLLMProvider : ILLMProvider
{
    private readonly ILLMProvider _inner;
    private readonly ILLMProvider _fallback;
    private readonly CircuitBreakerOptions _options;
    private readonly ILogger _logger;
    
    private CircuitBreakerState _state = CircuitBreakerState.Closed;
    private int _failureCount = 0;
    private DateTimeOffset _lastFailureTime;
    private readonly object _lock = new();
    
    public async Task<LLMResponse> CompleteAsync(LLMRequest request, CancellationToken ct)
    {
        lock (_lock)
        {
            if (_state == CircuitBreakerState.Open)
            {
                if (ShouldAttemptReset())
                {
                    _state = CircuitBreakerState.HalfOpen;
                    _logger.LogInformation("Circuit breaker entering half-open state");
                }
                else
                {
                    _logger.LogWarning("Circuit breaker open, routing to fallback");
                    return await _fallback.CompleteAsync(request, ct);
                }
            }
        }
        
        try
        {
            var response = await _inner.CompleteAsync(request, ct);
            OnSuccess();
            return response;
        }
        catch (Exception ex)
        {
            OnFailure(ex);
            
            lock (_lock)
            {
                if (_state == CircuitBreakerState.Open)
                {
                    _logger.LogWarning("Using fallback provider due to circuit breaker");
                    return await _fallback.CompleteAsync(request, ct);
                }
            }
            
            throw;
        }
    }
    
    private void OnSuccess()
    {
        lock (_lock)
        {
            _failureCount = 0;
            if (_state == CircuitBreakerState.HalfOpen)
            {
                _state = CircuitBreakerState.Closed;
                _logger.LogInformation("Circuit breaker closed after successful recovery");
            }
        }
    }
    
    private void OnFailure(Exception ex)
    {
        lock (_lock)
        {
            _failureCount++;
            _lastFailureTime = DateTimeOffset.UtcNow;
            
            if (_failureCount >= _options.FailureThreshold)
            {
                _state = CircuitBreakerState.Open;
                _logger.LogError(ex, "Circuit breaker opened after {Count} failures", _failureCount);
            }
        }
    }
    
    private bool ShouldAttemptReset()
    {
        return (DateTimeOffset.UtcNow - _lastFailureTime) >= _options.OpenDuration;
    }
}

public enum CircuitBreakerState
{
    Closed,    // Normal operation
    Open,      // Failing, use fallback
    HalfOpen   // Testing recovery
}
```

---

### 2. Streaming Architecture

#### 2.1 ILLMProvider Streaming Extension

**New Method:**
```csharp
public interface ILLMProvider
{
    // Existing
    Task<LLMResponse> CompleteAsync(LLMRequest request, CancellationToken ct);
    
    // NEW - Streaming support
    IAsyncEnumerable<string> CompleteStreamAsync(
        LLMRequest request, 
        [EnumeratorCancellation] CancellationToken ct);
}
```

#### 2.2 Streaming Implementation Pattern

**Azure OpenAI Streaming:**
```csharp
public async IAsyncEnumerable<string> CompleteStreamAsync(
    LLMRequest request,
    [EnumeratorCancellation] CancellationToken ct)
{
    var options = CreateChatCompletionsOptions(request);
    var streamingResponse = await _client.GetChatCompletionsStreamingAsync(
        _options.DeploymentName,
        options,
        ct);
    
    await foreach (var update in streamingResponse.WithCancellation(ct))
    {
        if (update.ContentUpdate != null)
        {
            yield return update.ContentUpdate;
        }
    }
}
```

#### 2.3 API Streaming (Server-Sent Events)

**Endpoint Design:**
```csharp
app.MapPost("/api/llm/complete/stream", async (
    [FromBody] LLMRequest request,
    ILLMProvider provider,
    HttpContext context,
    CancellationToken ct) =>
{
    context.Response.Headers.ContentType = "text/event-stream";
    context.Response.Headers.CacheControl = "no-cache";
    
    await foreach (var token in provider.CompleteStreamAsync(request, ct))
    {
        await context.Response.WriteAsync($"data: {token}\n\n", ct);
        await context.Response.Body.FlushAsync(ct);
    }
    
    await context.Response.WriteAsync("data: [DONE]\n\n", ct);
});
```

#### 2.4 Blazor Streaming (SignalR)

**Hub Design:**
```csharp
public class LLMStreamingHub : Hub
{
    private readonly ILLMProvider _provider;
    
    public async Task StreamCompletion(LLMRequest request)
    {
        await foreach (var token in _provider.CompleteStreamAsync(request, Context.ConnectionAborted))
        {
            await Clients.Caller.SendAsync("ReceiveToken", token);
        }
        
        await Clients.Caller.SendAsync("StreamComplete");
    }
}
```

**Client Usage:**
```csharp
@inject HubConnection HubConnection

private async Task StartStreaming()
{
    HubConnection.On<string>("ReceiveToken", token =>
    {
        generatedCode += token;
        StateHasChanged();
    });
    
    await HubConnection.SendAsync("StreamCompletion", request);
}
```

---

### 3. Security Architecture

#### 3.1 Azure Key Vault Integration

**Secret Provider Abstraction:**
```csharp
public interface ISecretProvider
{
    Task<string> GetSecretAsync(string secretName, CancellationToken ct = default);
    Task<Dictionary<string, string>> GetSecretsAsync(string[] secretNames, CancellationToken ct = default);
}

public class KeyVaultSecretProvider : ISecretProvider
{
    private readonly SecretClient _client;
    private readonly IMemoryCache _cache;
    private readonly ILogger _logger;
    
    public KeyVaultSecretProvider(
        string vaultUri,
        TokenCredential credential,
        IMemoryCache cache,
        ILogger<KeyVaultSecretProvider> logger)
    {
        _client = new SecretClient(new Uri(vaultUri), credential);
        _cache = cache;
        _logger = logger;
    }
    
    public async Task<string> GetSecretAsync(string secretName, CancellationToken ct)
    {
        // Check cache first
        if (_cache.TryGetValue(secretName, out string cachedValue))
        {
            return cachedValue;
        }
        
        // Fetch from Key Vault
        var secret = await _client.GetSecretAsync(secretName, cancellationToken: ct);
        
        // Cache for 5 minutes
        _cache.Set(secretName, secret.Value.Value, TimeSpan.FromMinutes(5));
        
        _logger.LogInformation("Retrieved secret {SecretName} from Key Vault", secretName);
        
        return secret.Value.Value;
    }
}
```

#### 3.2 Configuration Pattern

**Key Vault Reference Syntax:**
```json
{
  "AzureOpenAI": {
    "Endpoint": "@Microsoft.KeyVault(SecretUri=https://evoai-kv.vault.azure.net/secrets/OpenAIEndpoint)",
    "ApiKey": "@Microsoft.KeyVault(SecretUri=https://evoai-kv.vault.azure.net/secrets/OpenAIApiKey)"
  }
}
```

**Configuration Provider:**
```csharp
builder.Configuration.AddAzureKeyVault(
    new Uri(builder.Configuration["KeyVault:VaultUri"]),
    new DefaultAzureCredential());
```

**Managed Identity Setup:**
```csharp
// In production (Azure App Service)
var credential = new DefaultAzureCredential();

// In development
var credential = new DefaultAzureCredential(new DefaultAzureCredentialOptions
{
    ExcludeManagedIdentityCredential = true,
    ExcludeSharedTokenCacheCredential = true
});
```

---

## ?? Data Flow Diagrams

### Request Flow (with Routing)

```
????????????
? User App ?
????????????
     ? LLMRequest
     ?
???????????????????????????????????????????????????????????????
? RoutingLLMProvider                                           ?
?                                                              ?
? 1. Detect Task Type                                         ?
?    ?? Check metadata                                        ?
?    ?? Analyze system prompt                                 ?
?    ?? Analyze user prompt                                   ?
?                                                              ?
? 2. Select Route                                             ?
?    ?? Get routing strategy (TaskBased/CostOptimized)       ?
?    ?? Lookup primary provider for task type                ?
?    ?? Lookup fallback provider                             ?
?                                                              ?
? 3. Get Provider Instance                                    ?
?    ?? Resolve from service provider or factory             ?
???????????????????????????????????????????????????????????????
     ? Selected Provider
     ?
???????????????????????????????????????????????????????????????
? CircuitBreakerLLMProvider                                    ?
?                                                              ?
? 1. Check Circuit State                                      ?
?    ?? CLOSED: Execute primary                               ?
?    ?? OPEN: Use fallback immediately                        ?
?    ?? HALF-OPEN: Try primary (testing recovery)            ?
?                                                              ?
? 2. Execute Request                                           ?
?    ?? Try primary provider                                  ?
?    ?? On success: Reset failure counter                     ?
?    ?? On failure: Increment counter, maybe open circuit     ?
?                                                              ?
? 3. Handle Failure                                            ?
?    ?? If circuit opened: Use fallback                       ?
?    ?? Emit telemetry event                                  ?
???????????????????????????????????????????????????????????????
     ? Primary or Fallback Provider
     ?
???????????????????????????????????????????????????
? AzureOpenAI      ?   Ollama     ?    Claude     ?
? Provider         ?   Provider   ?    Provider   ?
???????????????????????????????????????????????????
     ? LLMResponse
     ?
????????????
? User App ?
????????????
```

### Streaming Flow

```
????????????
? User App ? ????????????
????????????            ? 1. StreamRequest
                        ?
????????????????????????????????????????????????????
? SSE Endpoint / SignalR Hub                       ?
????????????????????????????????????????????????????
                        ? 2. Call CompleteStreamAsync
                        ?
????????????????????????????????????????????????????
? RoutingLLMProvider                               ?
?  ?? Select appropriate provider for streaming    ?
????????????????????????????????????????????????????
                        ? 3. Get streaming provider
                        ?
????????????????????????????????????????????????????
? Provider (e.g., AzureOpenAIProvider)             ?
?                                                   ?
?  await foreach (var token in stream)             ?
?  {                                                ?
?      yield return token;  ?????????????          ?
?  }                                     ?          ?
?????????????????????????????????????????????????????
                        ?                ?
                        ? 4. Stream tokens back
                        ?                ?
?????????????????????????????????????????????????????
? SSE Endpoint / SignalR Hub             ?          ?
?                                        ?          ?
?  foreach token:                        ?          ?
?    Send to client ??????????????????????          ?
?    Flush buffer                                   ?
????????????????????????????????????????????????????
                        ? 5. SSE events / SignalR messages
                        ?
????????????????????????????????????????????????????
? User App (Blazor)                                ?
?                                                  ?
?  HubConnection.On<string>("ReceiveToken",        ?
?      token => UpdateUI(token));                  ?
????????????????????????????????????????????????????
```

---

## ?? Package Dependencies

### New Dependencies

```xml
<!-- Azure Key Vault -->
<PackageReference Include="Azure.Security.KeyVault.Secrets" Version="4.6.0" />
<PackageReference Include="Azure.Identity" Version="1.12.0" />

<!-- Configuration -->
<PackageReference Include="Microsoft.Extensions.Configuration.AzureKeyVault" Version="7.0.0" />

<!-- Streaming -->
<PackageReference Include="System.Threading.Channels" Version="8.0.0" />

<!-- SignalR (for Blazor streaming) -->
<PackageReference Include="Microsoft.AspNetCore.SignalR.Client" Version="10.0.0" />
```

### Existing Dependencies (Already in project)
- Microsoft.Extensions.Options
- Microsoft.Extensions.DependencyInjection
- Microsoft.Extensions.Logging
- Azure.AI.OpenAI
- System.Text.Json

---

## ?? Integration Points

### With Existing Components

| Component | Integration | Changes Needed |
|-----------|-------------|----------------|
| **ActionAnalyzerService** | Uses ILLMProvider | None - works automatically |
| **TestGeneratorService** | Uses ILLMProvider | Optional: Add streaming support |
| **RecordingAgent** | Uses ILLMProvider | None - works automatically |
| **PlannerAgent** | Uses ILLMProvider | Benefits from GPT-5 routing |
| **LLMProviderFactory** | Creates providers | Update to support RoutingLLMProvider |
| **ServiceCollectionExtensions** | DI registration | Add new services |

### Configuration Changes

**Before:**
```json
{
  "EvoAITest": {
    "Core": {
      "LLMProvider": "AzureOpenAI",
      "AzureOpenAI": {
        "Endpoint": "https://...",
        "ApiKey": "..."
      }
    }
  }
}
```

**After (Backward Compatible):**
```json
{
  "EvoAITest": {
    "Core": {
      "LLMProvider": "Routing",  // NEW: Enable routing
      "EnableMultiModelRouting": true,
      "RoutingStrategy": "TaskBased",
      "EnableProviderFallback": true,
      
      // Circuit breaker settings
      "CircuitBreakerFailureThreshold": 5,
      "CircuitBreakerOpenDurationSeconds": 30,
      
      // Routing configuration
      "Routing": {
        "Planning": {
          "PrimaryProvider": "AzureOpenAI",
          "PrimaryModel": "gpt-4",
          "FallbackProvider": "Ollama",
          "FallbackModel": "qwen2.5-7b"
        }
      }
    }
  }
}
```

---

## ?? Performance Considerations

### Latency Budget

| Operation | Target | P95 | P99 |
|-----------|--------|-----|-----|
| Task detection | < 5ms | 8ms | 12ms |
| Route selection | < 5ms | 8ms | 12ms |
| Circuit breaker check | < 1ms | 2ms | 3ms |
| Provider resolution | < 2ms | 4ms | 6ms |
| **Total Overhead** | **< 15ms** | **25ms** | **30ms** |

### Memory Considerations

- Circuit breaker state: ~1KB per provider
- Routing cache: ~10KB for 100 routes
- Secret cache: ~1KB per secret
- Total overhead: < 50MB for typical usage

### Caching Strategy

1. **Route Cache:** Cache routing decisions for 5 minutes
2. **Secret Cache:** Cache Key Vault secrets for 5 minutes
3. **Provider Cache:** Singleton providers, no per-request allocation

---

## ?? Security Considerations

### Secrets Management

1. **Never log secrets** - Redact in logging
2. **Rotate keys regularly** - Key Vault supports rotation
3. **Use managed identity** - No credentials in code
4. **Environment separation** - Separate vaults for dev/staging/prod
5. **Audit all access** - Key Vault provides audit logs

### Network Security

1. **HTTPS only** - All external API calls over TLS
2. **Private endpoints** - Use Azure Private Link for Key Vault
3. **Firewall rules** - Restrict Key Vault access by IP
4. **Rate limiting** - Prevent abuse of streaming endpoints

---

## ?? Monitoring & Observability

### Telemetry Events

```csharp
// Routing decision
_logger.LogInformation(
    "Routed {TaskType} to {Provider} (strategy: {Strategy})",
    taskType, providerName, strategy);

// Circuit breaker state change
_logger.LogWarning(
    "Circuit breaker opened for {Provider} after {Failures} failures",
    providerName, failureCount);

// Fallback usage
_logger.LogWarning(
    "Using fallback provider {Fallback} due to {Reason}",
    fallbackProvider, reason);

// Streaming metrics
_logger.LogInformation(
    "Streaming completed: {Tokens} tokens in {Duration}ms",
    tokenCount, duration);
```

### Metrics

- `llm.routing.decision` - Routing decisions by task type
- `llm.circuit_breaker.state` - Circuit breaker state changes
- `llm.provider.latency` - Latency per provider
- `llm.provider.errors` - Error rates per provider
- `llm.streaming.tokens` - Tokens streamed per request
- `llm.cost.tokens` - Token usage for cost tracking

---

**Document Status:** ? Complete  
**Next:** API Design Document  
**Owner:** Architecture Team
